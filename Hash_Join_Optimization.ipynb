{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391e6851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/shaiksaheer/anaconda3/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/shaiksaheer/anaconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/shaiksaheer/anaconda3/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/shaiksaheer/anaconda3/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shaiksaheer/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed40b7b",
   "metadata": {},
   "source": [
    "## Basic Hash Join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9e08a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read time: 2.5617 seconds\n",
      "Join time: 5.0788 seconds\n",
      "Total time: 7.6406 seconds\n",
      "Joined data has been saved to /Users/shaiksaheer/Documents/College_Docs/DataBaseDesign/V3.0.1/dbgen/Data/joined_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "def read_csv_data(file_path):\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        return list(csv.reader(csvfile))\n",
    "\n",
    "def hash_join(orders, lineitem):\n",
    "    hash_table = {order[0]: order for order in orders}\n",
    "\n",
    "    joined_data = []\n",
    "    for lineitem in lineitem:\n",
    "        order_key = lineitem[0]\n",
    "        if order_key in hash_table:\n",
    "            joined_data.append(hash_table[order_key] + lineitem)\n",
    "\n",
    "    return joined_data\n",
    "\n",
    "# Replace 'orders.csv' and 'lineitems.csv' with your actual file paths\n",
    "orders_path = '/Users/shaiksaheer/Documents/College_Docs/DataBaseDesign/V3.0.1/dbgen/Data/orders.csv'\n",
    "lineitems_path = '/Users/shaiksaheer/Documents/College_Docs/DataBaseDesign/V3.0.1/dbgen/Data/lineitem.csv'\n",
    "\n",
    "# Read in the data\n",
    "start_time = time.time()\n",
    "orders_data = read_csv_data(orders_path)\n",
    "lineitem_data = read_csv_data(lineitem_path)\n",
    "read_time = time.time() - start_time\n",
    "\n",
    "# Perform the hash join\n",
    "start_time = time.time()\n",
    "joined_data = hash_join(orders_data, lineitem_data)\n",
    "join_time = time.time() - start_time\n",
    "\n",
    "# Save the joined data to a new CSV file\n",
    "output_path = '/Users/shaiksaheer/Documents/College_Docs/DataBaseDesign/V3.0.1/dbgen/Data/joined_data.csv'\n",
    "with open(output_path, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in joined_data:\n",
    "        csvwriter.writerow(row)\n",
    "\n",
    "# Print out the performance metrics\n",
    "print(f\"Read time: {read_time:.4f} seconds\")\n",
    "print(f\"Join time: {join_time:.4f} seconds\")\n",
    "print(f\"Total time: {read_time + join_time:.4f} seconds\")\n",
    "print(f\"Joined data has been saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9e756d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash\n",
      "Successfully installed xxhash-3.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xxhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "352c8796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyfarmhash\n",
      "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pyfarmhash\n",
      "  Building wheel for pyfarmhash (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp310-cp310-macosx_11_0_arm64.whl size=11068 sha256=6a23ee97d818c8ab42850a2796e026894694f3cafeee89663007578daaacbbca\n",
      "  Stored in directory: /Users/shaiksaheer/Library/Caches/pip/wheels/a3/88/44/d0beeb16b34cbb3d7919ab0db836afe90c656bafd1034b5178\n",
      "Successfully built pyfarmhash\n",
      "Installing collected packages: pyfarmhash\n",
      "Successfully installed pyfarmhash-0.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyfarmhash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91bd4d",
   "metadata": {},
   "source": [
    "## Optimized XXhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07dc82ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read time: 2.7327 seconds\n",
      "Join time: 0.7529 seconds\n",
      "Total time: 3.4857 seconds\n",
      "Joined data has been saved to /Users/shaiksaheer/Documents/College_Docs/DataBaseDesign/V3.0.1/dbgen/Data/joined_data_xxhash_tuned.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import xxhash\n",
    "\n",
    "def read_csv_data_in_chunks(file_path, chunk_size=10000):\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        chunk = []\n",
    "        for i, row in enumerate(reader):\n",
    "            if i % chunk_size == 0 and i > 0:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "            chunk.append(row)\n",
    "        yield chunk\n",
    "\n",
    "def xxhash_key(value):\n",
    "    return xxhash.xxh64(value).intdigest()\n",
    "\n",
    "def hash_join(orders_chunk, lineitem, hash_function):\n",
    "    hash_table = {}\n",
    "    for order in orders_chunk:\n",
    "        hash_key = hash_function(order[0])\n",
    "        hash_table[hash_key] = order\n",
    "\n",
    "    joined_data = []\n",
    "    for item in lineitem:\n",
    "        hash_key = hash_function(item[0])\n",
    "        if hash_key in hash_table:\n",
    "            joined_data.append(hash_table[hash_key] + item)\n",
    "\n",
    "    return joined_data\n",
    "\n",
    "orders_path = '/Users/shaiksaheer/Documents/College_Docs/DataBaseDesign/V3.0.1/dbgen/Data/orders.csv'\n",
    "lineitem_path = '/Users/shaiksaheer/Documents/College_Docs/DataBaseDesign/V3.0.1/dbgen/Data/lineitem.csv'\n",
    "\n",
    "start_read_time = time.time()\n",
    "lineitem_data = list(read_csv_data_in_chunks(lineitem_path, chunk_size=10000))[0]\n",
    "read_time = time.time() - start_read_time\n",
    "\n",
    "start_join_time = time.time()\n",
    "joined_data = []\n",
    "for orders_chunk in read_csv_data_in_chunks(orders_path, chunk_size=10000):\n",
    "    joined_data.extend(hash_join(orders_chunk, lineitem_data, xxhash_key))\n",
    "join_time = time.time() - start_join_time\n",
    "\n",
    "output_path = '/Users/shaiksaheer/Documents/College_Docs/DataBaseDesign/V3.0.1/dbgen/Data/joined_data_xxhash_tuned.csv'\n",
    "with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in joined_data:\n",
    "        csvwriter.writerow(row)\n",
    "\n",
    "print(f\"Read time: {read_time:.4f} seconds\")\n",
    "print(f\"Join time: {join_time:.4f} seconds\")\n",
    "print(f\"Total time: {read_time + join_time:.4f} seconds\")\n",
    "print(f\"Joined data has been saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841bcf5",
   "metadata": {},
   "source": [
    "## Optimized FarmHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfef7e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read time: 5.4128 seconds\n",
      "Join time: 0.6477 seconds\n",
      "Total time: 6.0605 seconds\n",
      "Joined data has been saved to /Users/shaiksaheer/Documents/College_Docs/DataBaseDesign/V3.0.1/dbgen/Data/joined_data_farmhash_tuned.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import farmhash\n",
    "\n",
    "def read_csv_data_in_chunks(file_path, chunk_size=10000):\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        chunk = []\n",
    "        for i, row in enumerate(reader):\n",
    "            if i % chunk_size == 0 and i > 0:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "            chunk.append(row)\n",
    "        yield chunk\n",
    "\n",
    "def farmhash_key(value):\n",
    "    return farmhash.hash64(value)\n",
    "\n",
    "def hash_join(orders_chunk, lineitem, hash_function):\n",
    "    hash_table = {}\n",
    "    for order in orders_chunk:\n",
    "        hash_key = hash_function(order[0])\n",
    "        hash_table[hash_key] = order\n",
    "\n",
    "    joined_data = []\n",
    "    for item in lineitem:\n",
    "        hash_key = hash_function(item[0])\n",
    "        if hash_key in hash_table:\n",
    "            joined_data.append(hash_table[hash_key] + item)\n",
    "\n",
    "    return joined_data\n",
    "\n",
    "orders_path = '/Users/shaiksaheer/Documents/College_Docs/DataBaseDesign/V3.0.1/dbgen/Data/orders.csv'\n",
    "lineitem_path = '/Users/shaiksaheer/Documents/College_Docs/DataBaseDesign/V3.0.1/dbgen/Data/lineitem.csv'\n",
    "\n",
    "start_read_time = time.time()\n",
    "lineitem_data = list(read_csv_data_in_chunks(lineitem_path, chunk_size=10000))[0]  # Read the first chunk for demonstration\n",
    "\n",
    "# Perform the hash join in chunks\n",
    "start_join_time = time.time()\n",
    "joined_data = []\n",
    "for orders_chunk in read_csv_data_in_chunks(orders_path, chunk_size=10000):\n",
    "    joined_data.extend(hash_join(orders_chunk, lineitem_data, farmhash_key))\n",
    "join_time = time.time() - start_join_time\n",
    "\n",
    "output_path = '/Users/shaiksaheer/Documents/College_Docs/DataBaseDesign/V3.0.1/dbgen/Data/joined_data_farmhash_tuned.csv'\n",
    "with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in joined_data:\n",
    "        csvwriter.writerow(row)\n",
    "\n",
    "read_time = time.time() - start_read_time - join_time\n",
    "print(f\"Read time: {read_time:.4f} seconds\")\n",
    "print(f\"Join time: {join_time:.4f} seconds\")\n",
    "print(f\"Total time: {read_time + join_time:.4f} seconds\")\n",
    "print(f\"Joined data has been saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
